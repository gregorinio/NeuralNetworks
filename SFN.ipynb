{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: pylab import has clobbered these variables: ['convolve']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import skimage.io\n",
    "from scipy.ndimage.filters import convolve\n",
    "\n",
    "#note: this requires the starter code for the assignments!\n",
    "from common.plotting import plot_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fix a bug in printing SVG\n",
    "if sys.platform == 'win32':\n",
    "    print \"Monkey-patching pydot\"\n",
    "    import pydot\n",
    "\n",
    "    def force_find_graphviz(graphviz_root):\n",
    "        binpath = os.path.join(graphviz_root, 'bin')\n",
    "        programs = 'dot twopi neato circo fdp sfdp'\n",
    "        def helper():\n",
    "            for prg in programs.split():\n",
    "                if os.path.exists(os.path.join(binpath, prg)):\n",
    "                    yield ((prg, os.path.join(binpath, prg)))\n",
    "                elif os.path.exists(os.path.join(binpath, prg+'.exe')):\n",
    "                    yield ((prg, os.path.join(binpath, prg+'.exe')))\n",
    "        progdict = dict(helper())\n",
    "        return lambda: progdict\n",
    "\n",
    "    pydot.find_graphviz = force_find_graphviz('c:/Program Files (x86)/Graphviz2.34/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "import theano.tensor.signal.downsample\n",
    "import theano.tensor.nlinalg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import SVG\n",
    "def svgdotprint(g):\n",
    "    return SVG(theano.printing.pydotprint(g, return_image=True, format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get grayscale we will simply average over the last dimension (channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def max_pool(img, nr, nc):\n",
    "    im_r, im_c = img.shape\n",
    "    im_ret = np.zeros((np.floor(im_r/nr), np.floor(im_c/nc)))\n",
    "    for r in range(im_ret.shape[0]):\n",
    "        for c in range(im_ret.shape[1]):\n",
    "            im_ret[r,c] = img[r*nr:(r+1)*nr, c*nc:(c+1)*nc].max()\n",
    "    return im_ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from fuel.datasets.mnist import MNIST\n",
    "from fuel.transformers import ScaleAndShift, Cast, Flatten, Mapping\n",
    "from fuel.streams import DataStream\n",
    "from fuel.schemes import SequentialScheme, ShuffledScheme\n",
    "\n",
    "MNIST.default_transformers = (\n",
    "    (ScaleAndShift, [2.0 / 255.0, -1], {'which_sources': 'features'}),\n",
    "    (Cast, [np.float32], {'which_sources': 'features'}))\n",
    "\n",
    "mnist_train = MNIST((\"train\",), subset=slice(None,50000))\n",
    "#this stream will shuffle the MNIST set and return us batches of 100 examples\n",
    "mnist_train_stream = DataStream.default_stream(\n",
    "    mnist_train,\n",
    "    iteration_scheme=ShuffledScheme(mnist_train.num_examples, 25))\n",
    "                                               \n",
    "mnist_validation = MNIST((\"train\",), subset=slice(50000, None))\n",
    "\n",
    "# We will use larger portions for testing and validation\n",
    "# as these dont do a backward pass and reauire less RAM.\n",
    "mnist_validation_stream = DataStream.default_stream(\n",
    "    mnist_validation, iteration_scheme=SequentialScheme(mnist_validation.num_examples, 100))\n",
    "mnist_test = MNIST((\"test\",))\n",
    "mnist_test_stream = DataStream.default_stream(\n",
    "    mnist_test, iteration_scheme=SequentialScheme(mnist_test.num_examples, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The streams return batches containing (u'features', u'targets')\n",
      "Each trainin batch consits of a tuple containing:\n",
      " - an array of size (25, 1, 28, 28) containing float32\n",
      " - an array of size (25, 1) containing uint8\n",
      "Validation/test batches consits of tuples containing:\n",
      " - an array of size (100, 1, 28, 28) containing float32\n",
      " - an array of size (100, 1) containing uint8\n"
     ]
    }
   ],
   "source": [
    "print \"The streams return batches containing %s\" % (mnist_train_stream.sources,)\n",
    "\n",
    "print \"Each trainin batch consits of a tuple containing:\"\n",
    "for element in next(mnist_train_stream.get_epoch_iterator()):\n",
    "    print \" - an array of size %s containing %s\" % (element.shape, element.dtype)\n",
    "    \n",
    "print \"Validation/test batches consits of tuples containing:\"\n",
    "for element in next(mnist_test_stream.get_epoch_iterator()):\n",
    "    print \" - an array of size %s containing %s\" % (element.shape, element.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# These are taken from https://github.com/mila-udem/blocks\n",
    "# \n",
    "\n",
    "class Constant():\n",
    "    \"\"\"Initialize parameters to a constant.\n",
    "    The constant may be a scalar or a :class:`~numpy.ndarray` of any shape\n",
    "    that is broadcastable with the requested parameter arrays.\n",
    "    Parameters\n",
    "    ----------\n",
    "    constant : :class:`~numpy.ndarray`\n",
    "        The initialization value to use. Must be a scalar or an ndarray (or\n",
    "        compatible object, such as a nested list) that has a shape that is\n",
    "        broadcastable with any shape requested by `initialize`.\n",
    "    \"\"\"\n",
    "    def __init__(self, constant):\n",
    "        self._constant = numpy.asarray(constant)\n",
    "\n",
    "    def generate(self, rng, shape):\n",
    "        dest = numpy.empty(shape, dtype=np.float32)\n",
    "        dest[...] = self._constant\n",
    "        return dest\n",
    "\n",
    "\n",
    "class IsotropicGaussian():\n",
    "    \"\"\"Initialize parameters from an isotropic Gaussian distribution.\n",
    "    Parameters\n",
    "    ----------\n",
    "    std : float, optional\n",
    "        The standard deviation of the Gaussian distribution. Defaults to 1.\n",
    "    mean : float, optional\n",
    "        The mean of the Gaussian distribution. Defaults to 0\n",
    "    Notes\n",
    "    -----\n",
    "    Be careful: the standard deviation goes first and the mean goes\n",
    "    second!\n",
    "    \"\"\"\n",
    "    def __init__(self, std=1, mean=0):\n",
    "        self._mean = mean\n",
    "        self._std = std\n",
    "\n",
    "    def generate(self, rng, shape):\n",
    "        m = rng.normal(self._mean, self._std, size=shape)\n",
    "        return m.astype(np.float32)\n",
    "\n",
    "\n",
    "class Uniform():\n",
    "    \"\"\"Initialize parameters from a uniform distribution.\n",
    "    Parameters\n",
    "    ----------\n",
    "    mean : float, optional\n",
    "        The mean of the uniform distribution (i.e. the center of mass for\n",
    "        the density function); Defaults to 0.\n",
    "    width : float, optional\n",
    "        One way of specifying the range of the uniform distribution. The\n",
    "        support will be [mean - width/2, mean + width/2]. **Exactly one**\n",
    "        of `width` or `std` must be specified.\n",
    "    std : float, optional\n",
    "        An alternative method of specifying the range of the uniform\n",
    "        distribution. Chooses the width of the uniform such that random\n",
    "        variates will have a desired standard deviation. **Exactly one** of\n",
    "        `width` or `std` must be specified.\n",
    "    \"\"\"\n",
    "    def __init__(self, mean=0., width=None, std=None):\n",
    "        if (width is not None) == (std is not None):\n",
    "            raise ValueError(\"must specify width or std, \"\n",
    "                             \"but not both\")\n",
    "        if std is not None:\n",
    "            # Variance of a uniform is 1/12 * width^2\n",
    "            self._width = numpy.sqrt(12) * std\n",
    "        else:\n",
    "            self._width = width\n",
    "        self._mean = mean\n",
    "\n",
    "    def generate(self, rng, shape):\n",
    "        w = self._width / 2\n",
    "        m = rng.uniform(self._mean - w, self._mean + w, size=shape)\n",
    "        return m.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (3, 1, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "# A theano variable is an entry to the cmputational graph\n",
    "# We will need to provide its value during function call\n",
    "# X is batch_size x num_channels x img_rows x img_columns\n",
    "X = theano.tensor.tensor4('X')\n",
    "\n",
    "# Y is 1D, it lists the targets for all examples\n",
    "Y = theano.tensor.matrix('Y', dtype='uint8')\n",
    "\n",
    "#The tag values are useful during debugging the creation of Theano graphs\n",
    "\n",
    "X_test_value, Y_test_value = next(mnist_train_stream.get_epoch_iterator())\n",
    "#\n",
    "# Unfortunately, test tags don't work with convolutions with newest Theano :(\n",
    "#\n",
    "theano.config.compute_test_value = 'off' # Enable the computation of test values\n",
    "\n",
    "\n",
    "X.tag.test_value = X_test_value[:3]\n",
    "Y.tag.test_value = Y_test_value[:3]\n",
    "\n",
    "print \"X shape: %s\" % (X.tag.test_value.shape,)\n",
    "\n",
    "# this list will hold all parameters of the network\n",
    "model_parameters = []\n",
    "flat_parameters  = []\n",
    "\n",
    "#The first convolutional layer\n",
    "#The shape is: num_out_filters x num_in_filters x filter_height x filter_width\n",
    "num_filters_1 = 1 #1 #3 #10 #we will apply that many convolution filters in the first layer\n",
    "CW1_flat = theano.shared(np.zeros((num_filters_1*1*5*5), dtype='float32'),\n",
    "                   name='CW1_flat')\n",
    "CW1 = CW1_flat.reshape((num_filters_1,1,5,5))\n",
    "CW1_flat.tag.initializer = IsotropicGaussian(0.05)\n",
    "\n",
    "CB1 = theano.shared(np.zeros((num_filters_1,), dtype='float32'),\n",
    "                    name='CB1')\n",
    "CB1.tag.initializer = Constant(0.0)\n",
    "model_parameters += [CW1, CB1]\n",
    "flat_parameters  += [CW1_flat, CB1]\n",
    "\n",
    "after_C1 = theano.tensor.maximum(\n",
    "    0.0,\n",
    "    theano.tensor.nnet.conv2d(X, CW1) + CB1.dimshuffle('x',0,'x','x')\n",
    "    )\n",
    "# print \"after_C1 shape: %s\" % (after_C1.tag.test_value.shape,)\n",
    "after_P1 = theano.tensor.signal.downsample.max_pool_2d(after_C1, (2,2), ignore_border=True) # bylo (2,2)\n",
    "# print \"after_P1 shape: %s\" % (after_P1.tag.test_value.shape,)\n",
    "\n",
    "\n",
    "num_filters_2 = 1 #3 #3 #25 #we will compute ten convolution filters in the first layer\n",
    "\n",
    "CW2_flat = theano.shared(np.zeros((num_filters_2*num_filters_1*5*5), dtype='float32'),\n",
    "                   name='CW2_flat')\n",
    "CW2 = CW2_flat.reshape((num_filters_2,num_filters_1,5,5))\n",
    "CW2_flat.tag.initializer = IsotropicGaussian(0.05)\n",
    "\n",
    "\n",
    "CB2 = theano.shared(np.zeros((num_filters_2,), dtype='float32'),\n",
    "                    name='CB2')\n",
    "CB2.tag.initializer = Constant(0.0)\n",
    "model_parameters += [CW2, CB2]\n",
    "flat_parameters  += [CW2_flat, CB2]\n",
    "\n",
    "after_C2 = theano.tensor.maximum(\n",
    "    0.0,\n",
    "    theano.tensor.nnet.conv2d(after_P1, CW2) + CB2.dimshuffle('x',0,'x','x')\n",
    "    )\n",
    "after_P2 = theano.tensor.signal.downsample.max_pool_2d(after_C2, (2,2), ignore_border=True) # bylo (2,2)\n",
    "\n",
    "\n",
    "num_fw3_hidden= 12 #100 #was 500 !!!!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "FW3_flat = theano.shared(np.zeros((num_filters_2*4*4*num_fw3_hidden), dtype='float32'),\n",
    "                   name='FW3_flat')\n",
    "FW3 = FW3_flat.reshape((num_filters_2*4*4,num_fw3_hidden))\n",
    "FW3_flat.tag.initializer = IsotropicGaussian(0.05)\n",
    "\n",
    "FB3 = theano.shared(np.zeros((num_fw3_hidden,), dtype='float32'),\n",
    "                    name='FB3')\n",
    "FB3.tag.initializer = Constant(0.0)\n",
    "model_parameters += [FW3, FB3]\n",
    "flat_parameters  += [FW3_flat, FB3]\n",
    "\n",
    "after_F3 = theano.tensor.maximum(0.0, \n",
    "                                 theano.tensor.dot(after_P2.flatten(2), FW3) + FB3.dimshuffle('x',0))\n",
    "\n",
    "\n",
    "num_fw4_hidden=10\n",
    "\n",
    "FW4_flat = theano.shared(np.zeros((num_fw3_hidden*num_fw4_hidden), dtype='float32'),\n",
    "                   name='FW4_flat')\n",
    "FW4 = FW4_flat.reshape((num_fw3_hidden, num_fw4_hidden))\n",
    "FW4_flat.tag.initializer = IsotropicGaussian(0.05)\n",
    "\n",
    "FB4 = theano.shared(np.zeros((num_fw4_hidden,), dtype='float32'),\n",
    "                    name='FB4')\n",
    "FB4.tag.initializer = Constant(0.0)\n",
    "model_parameters += [FW4, FB4]\n",
    "flat_parameters  += [FW4_flat, FB4]\n",
    "\n",
    "after_F4 = theano.tensor.dot(after_F3, FW4) + FB4.dimshuffle('x',0)\n",
    "\n",
    "log_probs = theano.tensor.nnet.softmax(after_F4)\n",
    "\n",
    "predictions = theano.tensor.argmax(log_probs, axis=1)\n",
    "\n",
    "error_rate = theano.tensor.neq(predictions,Y.ravel()).mean()\n",
    "nll = - theano.tensor.log(log_probs[theano.tensor.arange(Y.shape[0]), Y.ravel()]).mean()\n",
    "\n",
    "weight_decay = 0.0\n",
    "for p in flat_parameters:\n",
    "    if p.name[1]=='W':\n",
    "        weight_decay = weight_decay + 1e-3 * (p**2).sum()\n",
    "\n",
    "cost = nll + weight_decay\n",
    "\n",
    "#At this point stop computing test values\n",
    "theano.config.compute_test_value = 'off' # Enable the computation of test values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#\n",
    "# We have built a computation graph for computing the error_rate, predictions and cost\n",
    "#\n",
    "#svgdotprint(nll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The updates will update our shared values\n",
    "updates = []\n",
    "updates2 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lrate = theano.tensor.scalar('lrate',dtype='float32')\n",
    "momentum = theano.tensor.scalar('momentum',dtype='float32')\n",
    "epocnt = theano.tensor.scalar('epocnt',dtype='float32')\n",
    "\n",
    "# Theano will compute the gradients for us\n",
    "gradients = theano.grad(cost, flat_parameters)\n",
    "\n",
    "hs = theano.gradient.hessian(cost, flat_parameters)\n",
    "\n",
    "#initialize storage for momentum\n",
    "velocities = [theano.shared(np.zeros_like(p.get_value()), name='V_%s' %(p.name, )) for p in flat_parameters]\n",
    "\n",
    "for p,g,v,h in zip(flat_parameters, gradients, velocities, hs):\n",
    "    v_new = momentum * v - lrate * g\n",
    "    p_new = p + v_new\n",
    "    updates += [(v,v_new), (p, p_new)]\n",
    "\n",
    "#eigvall = []    \n",
    "delta1 = 1e-4\n",
    "delta2 = 1e-3\n",
    "    \n",
    "for p,g,v,h in zip(flat_parameters, gradients, velocities, hs):\n",
    "    #print v.shape, g.shape, \"after_F3 shape: %s\" % (p.tag.test_value.shape,)\n",
    "    v_new = momentum * v - lrate * g\n",
    "    eigv, eigm = theano.tensor.nlinalg.eig(h) # lub Eigh dla sym macierzy\n",
    "    hinv = theano.tensor.dot(eigm.T, theano.tensor.dot(theano.tensor.nlinalg.matrix_inverse(\n",
    "                (theano.tensor.nlinalg.alloc_diag(theano.tensor.abs_(eigv)))), eigm))\n",
    "    dtheta = theano.tensor.dot(g, hinv)\n",
    "    alpha = theano.tensor.min(delta1 / theano.tensor.dot(dtheta.T, theano.tensor.dot(h, dtheta)), delta2)\n",
    "    dtheta = alpha * dtheta\n",
    "    p_new = p - lrate * dtheta\n",
    "    updates2 += [(v,v_new), (p, p_new)]\n",
    "    \n",
    "#eigvall = theano.tensor.sort(eigvall)\n",
    "#eigmin = theano.tensor.min(eigvall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(V_CW1_flat, Elemwise{sub,no_inplace}.0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(V_CW1_flat, Elemwise{sub,no_inplace}.0),\n",
       " (CW1_flat, Elemwise{add,no_inplace}.0),\n",
       " (V_CB1, Elemwise{sub,no_inplace}.0),\n",
       " (CB1, Elemwise{add,no_inplace}.0),\n",
       " (V_CW2_flat, Elemwise{sub,no_inplace}.0),\n",
       " (CW2_flat, Elemwise{add,no_inplace}.0),\n",
       " (V_CB2, Elemwise{sub,no_inplace}.0),\n",
       " (CB2, Elemwise{add,no_inplace}.0),\n",
       " (V_FW3_flat, Elemwise{sub,no_inplace}.0),\n",
       " (FW3_flat, Elemwise{add,no_inplace}.0),\n",
       " (V_FB3, Elemwise{sub,no_inplace}.0),\n",
       " (FB3, Elemwise{add,no_inplace}.0),\n",
       " (V_FW4_flat, Elemwise{sub,no_inplace}.0),\n",
       " (FW4_flat, Elemwise{add,no_inplace}.0),\n",
       " (V_FB4, Elemwise{sub,no_inplace}.0),\n",
       " (FB4, Elemwise{add,no_inplace}.0)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print updates[0]\n",
    "updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#compile theano functions\n",
    "\n",
    "#each call to train step will make one SGD step\n",
    "train_step = theano.function([X,Y,lrate,momentum],[cost, error_rate, nll, weight_decay],updates=updates, on_unused_input='warn')\n",
    "train_step2 = theano.function([X,Y,lrate,momentum],[cost, error_rate, nll, weight_decay],updates=updates2, on_unused_input='warn')\n",
    "#each call to predict will return predictions on a batch of data\n",
    "predict = theano.function([X], predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_error_rate(stream):\n",
    "    errs = 0.0\n",
    "    num_samples = 0.0\n",
    "    for X, Y in stream.get_epoch_iterator():\n",
    "        errs += (predict(X)!=Y.ravel()).sum()\n",
    "        num_samples += Y.shape[0]\n",
    "    return errs/num_samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#utilities to save values of parameters and to load them\n",
    "\n",
    "def init_parameters():\n",
    "    rng = np.random.RandomState(1234)\n",
    "    for p in flat_parameters:\n",
    "        p.set_value(p.tag.initializer.generate(rng, p.get_value().shape))\n",
    "\n",
    "def snapshot_parameters():\n",
    "    return [p.get_value(borrow=False) for p in flat_parameters]\n",
    "\n",
    "def load_parameters(snapshot):\n",
    "    for p, s in zip(flat_parameters, snapshot):\n",
    "        p.set_value(s, borrow=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# init training\n",
    "\n",
    "i=0\n",
    "e=0\n",
    "\n",
    "init_parameters()\n",
    "for v in velocities:\n",
    "    v.set_value(np.zeros_like(v.get_value()))\n",
    "\n",
    "best_valid_error_rate = np.inf\n",
    "best_params = snapshot_parameters()\n",
    "best_params_epoch = 0\n",
    "\n",
    "train_erros = []\n",
    "train_loss = []\n",
    "train_nll = []\n",
    "validation_errors = []\n",
    "\n",
    "number_of_epochs = 3\n",
    "patience_expansion = 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At minibatch 100, batch loss 2.301771, batch nll 2.300945, batch error rate 88.000000%\n",
      "At minibatch 200, batch loss 2.310752, batch nll 2.309930, batch error rate 92.000000%\n",
      "At minibatch 300, batch loss 2.305181, batch nll 2.304360, batch error rate 88.000000%\n",
      "At minibatch 400, batch loss 2.288456, batch nll 2.287631, batch error rate 80.000000%\n",
      "At minibatch 500, batch loss 2.292042, batch nll 2.291196, batch error rate 76.000000%\n",
      "At minibatch 600, batch loss 2.298955, batch nll 2.298061, batch error rate 92.000000%\n",
      "At minibatch 700, batch loss 2.312648, batch nll 2.311620, batch error rate 100.000000%\n",
      "At minibatch 800, batch loss 2.254777, batch nll 2.252944, batch error rate 96.000000%\n",
      "At minibatch 900, batch loss 1.103641, batch nll 1.096123, batch error rate 40.000000%\n",
      "At minibatch 1000, batch loss 0.871867, batch nll 0.861703, batch error rate 28.000000%\n",
      "At minibatch 1100, batch loss 0.648552, batch nll 0.635762, batch error rate 24.000000%\n",
      "At minibatch 1200, batch loss 1.095072, batch nll 1.080308, batch error rate 36.000000%\n",
      "At minibatch 1300, batch loss 0.753582, batch nll 0.736538, batch error rate 24.000000%\n",
      "At minibatch 1400, batch loss 0.536046, batch nll 0.517249, batch error rate 16.000000%\n",
      "At minibatch 1500, batch loss 0.504739, batch nll 0.484553, batch error rate 16.000000%\n",
      "At minibatch 1600, batch loss 0.406707, batch nll 0.385270, batch error rate 12.000000%\n",
      "At minibatch 1700, batch loss 1.003088, batch nll 0.980691, batch error rate 36.000000%\n",
      "At minibatch 1800, batch loss 0.619093, batch nll 0.595306, batch error rate 24.000000%\n",
      "At minibatch 1900, batch loss 0.587195, batch nll 0.562522, batch error rate 24.000000%\n",
      "At minibatch 2000, batch loss 0.784647, batch nll 0.758903, batch error rate 28.000000%\n",
      "After epoch 1: valid_err_rate: 15.170000% currently going to do 3 epochs\n",
      "After epoch 1: averaged train_err_rate: 52.138000% averaged train nll: 1.412632 averaged train loss: 1.423353\n",
      "At minibatch 2100, batch loss 0.633225, batch nll 0.606624, batch error rate 16.000000%\n",
      "At minibatch 2200, batch loss 0.276785, batch nll 0.249617, batch error rate 4.000000%\n",
      "At minibatch 2300, batch loss 0.288096, batch nll 0.260260, batch error rate 12.000000%\n",
      "At minibatch 2400, batch loss 0.911881, batch nll 0.883431, batch error rate 28.000000%\n",
      "At minibatch 2500, batch loss 0.453738, batch nll 0.424909, batch error rate 24.000000%\n",
      "At minibatch 2600, batch loss 0.444825, batch nll 0.415560, batch error rate 8.000000%\n",
      "At minibatch 2700, batch loss 0.389933, batch nll 0.360352, batch error rate 12.000000%\n",
      "At minibatch 2800, batch loss 0.487652, batch nll 0.457506, batch error rate 16.000000%\n",
      "At minibatch 2900, batch loss 0.573475, batch nll 0.543153, batch error rate 24.000000%\n",
      "At minibatch 3000, batch loss 0.427208, batch nll 0.396385, batch error rate 12.000000%\n",
      "At minibatch 3100, batch loss 0.788974, batch nll 0.757798, batch error rate 16.000000%\n",
      "At minibatch 3200, batch loss 0.236477, batch nll 0.205209, batch error rate 4.000000%\n",
      "At minibatch 3300, batch loss 0.577302, batch nll 0.545694, batch error rate 20.000000%\n",
      "At minibatch 3400, batch loss 0.430290, batch nll 0.398401, batch error rate 16.000000%\n",
      "At minibatch 3500, batch loss 0.285422, batch nll 0.253687, batch error rate 16.000000%\n",
      "At minibatch 3600, batch loss 0.546937, batch nll 0.515038, batch error rate 20.000000%\n",
      "At minibatch 3700, batch loss 0.476683, batch nll 0.444638, batch error rate 16.000000%\n",
      "At minibatch 3800, batch loss 0.408869, batch nll 0.376633, batch error rate 16.000000%\n",
      "At minibatch 3900, batch loss 0.554424, batch nll 0.521749, batch error rate 20.000000%\n",
      "At minibatch 4000, batch loss 0.415302, batch nll 0.382641, batch error rate 8.000000%\n",
      "After epoch 2: valid_err_rate: 12.120000% currently going to do 4 epochs\n",
      "After epoch 2: averaged train_err_rate: 14.482000% averaged train nll: 0.453398 averaged train loss: 0.483646\n",
      "At minibatch 4100, batch loss 0.372397, batch nll 0.339494, batch error rate 4.000000%\n",
      "At minibatch 4200, batch loss 0.319307, batch nll 0.286263, batch error rate 8.000000%\n",
      "At minibatch 4300, batch loss 0.630362, batch nll 0.597344, batch error rate 12.000000%\n",
      "At minibatch 4400, batch loss 0.294493, batch nll 0.261407, batch error rate 12.000000%\n",
      "At minibatch 4500, batch loss 0.264869, batch nll 0.231547, batch error rate 4.000000%\n",
      "At minibatch 4600, batch loss 0.347502, batch nll 0.314019, batch error rate 12.000000%\n",
      "At minibatch 4700, batch loss 0.402706, batch nll 0.369191, batch error rate 8.000000%\n",
      "At minibatch 4800, batch loss 0.195054, batch nll 0.161599, batch error rate 4.000000%\n",
      "At minibatch 4900, batch loss 0.248627, batch nll 0.215052, batch error rate 8.000000%\n",
      "At minibatch 5000, batch loss 0.282477, batch nll 0.248808, batch error rate 12.000000%\n",
      "At minibatch 5100, batch loss 0.502913, batch nll 0.469288, batch error rate 16.000000%\n",
      "At minibatch 5200, batch loss 0.465440, batch nll 0.431702, batch error rate 20.000000%\n",
      "At minibatch 5300, batch loss 0.349140, batch nll 0.315604, batch error rate 8.000000%\n",
      "At minibatch 5400, batch loss 0.422513, batch nll 0.388713, batch error rate 16.000000%\n",
      "At minibatch 5500, batch loss 0.816707, batch nll 0.782835, batch error rate 24.000000%\n",
      "At minibatch 5600, batch loss 0.462447, batch nll 0.428797, batch error rate 16.000000%\n",
      "At minibatch 5700, batch loss 0.351036, batch nll 0.317428, batch error rate 12.000000%\n",
      "At minibatch 5800, batch loss 0.228771, batch nll 0.195195, batch error rate 4.000000%\n",
      "At minibatch 5900, batch loss 0.328958, batch nll 0.295186, batch error rate 4.000000%\n",
      "At minibatch 6000, batch loss 0.815874, batch nll 0.782201, batch error rate 20.000000%\n",
      "After epoch 3: valid_err_rate: 10.210000% currently going to do 5 epochs\n",
      "After epoch 3: averaged train_err_rate: 12.132000% averaged train nll: 0.388267 averaged train loss: 0.421731\n",
      "..."
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "epocnt = 1\n",
    "\n",
    "while e<number_of_epochs: #This loop goes over epochs\n",
    "    e += 1\n",
    "    #First train on all data from this batch\n",
    "    epoch_start_i = i\n",
    "    for X_batch, Y_batch in mnist_train_stream.get_epoch_iterator(): \n",
    "        i += 1\n",
    "        \n",
    "        K = 2000\n",
    "        lrate = 4e-3 * K / np.maximum(K, i)\n",
    "        momentum=0.9\n",
    "        \n",
    "        L, err_rate, nll, wdec = train_step(X_batch, Y_batch, lrate, momentum)\n",
    "        \n",
    "        train_loss.append((i,L))\n",
    "        train_erros.append((i,err_rate))\n",
    "        train_nll.append((i,nll))\n",
    "        #if i % 10000 == 0:\n",
    "        #    print eigen(X_batch, Y_batch)\n",
    "        if i % 100 == 0:\n",
    "            print \"At minibatch %d, batch loss %f, batch nll %f, batch error rate %f%%\" % (i, L, nll, err_rate*100)\n",
    "        \n",
    "    # After an epoch compute validation error\n",
    "    val_error_rate = compute_error_rate(mnist_validation_stream)\n",
    "    if val_error_rate < best_valid_error_rate:\n",
    "        number_of_epochs = np.maximum(number_of_epochs, e * patience_expansion+1)\n",
    "        best_valid_error_rate = val_error_rate\n",
    "        best_params = snapshot_parameters()\n",
    "        best_params_epoch = e\n",
    "    validation_errors.append((i,val_error_rate))\n",
    "    print \"After epoch %d: valid_err_rate: %f%% currently going to do %d epochs\" %(\n",
    "        e, val_error_rate*100, number_of_epochs)\n",
    "    print \"After epoch %d: averaged train_err_rate: %f%% averaged train nll: %f averaged train loss: %f\" %(\n",
    "        e, np.mean(np.asarray(train_erros)[epoch_start_i:,1])*100, \n",
    "        np.mean(np.asarray(train_nll)[epoch_start_i:,1]),\n",
    "        np.mean(np.asarray(train_loss)[epoch_start_i:,1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At minibatch 416100, batch loss 0.302119, batch nll 0.270876, batch error rate 12.000000%\n",
      "At minibatch 416200, batch loss 0.131105, batch nll 0.099862, batch error rate 0.000000%\n",
      "At minibatch 416300, batch loss 0.226554, batch nll 0.195312, batch error rate 8.000000%\n",
      "At minibatch 416400, batch loss 0.324439, batch nll 0.293197, batch error rate 4.000000%\n",
      "At minibatch 416500, batch loss 0.085683, batch nll 0.054441, batch error rate 0.000000%\n",
      "At minibatch 416600, batch loss 0.320017, batch nll 0.288775, batch error rate 4.000000%\n",
      "At minibatch 416700, batch loss 0.175342, batch nll 0.144100, batch error rate 0.000000%\n",
      "At minibatch 416800, batch loss 0.378575, batch nll 0.347333, batch error rate 8.000000%\n",
      "At minibatch 416900, batch loss 0.158352, batch nll 0.127109, batch error rate 4.000000%\n",
      "At minibatch 417000, batch loss 0.330239, batch nll 0.298996, batch error rate 8.000000%\n",
      "At minibatch 417100, batch loss 0.272431, batch nll 0.241188, batch error rate 8.000000%\n",
      "At minibatch 417200, batch loss 0.159267, batch nll 0.128025, batch error rate 4.000000%\n",
      "At minibatch 417300, batch loss 0.379496, batch nll 0.348253, batch error rate 12.000000%\n",
      "At minibatch 417400, batch loss 0.380440, batch nll 0.349198, batch error rate 16.000000%\n",
      "At minibatch 417500, batch loss 0.602883, batch nll 0.571641, batch error rate 16.000000%\n",
      "At minibatch 417600, batch loss 0.426845, batch nll 0.395602, batch error rate 12.000000%\n",
      "At minibatch 417700, batch loss 0.456930, batch nll 0.425688, batch error rate 16.000000%\n",
      "At minibatch 417800, batch loss 0.207233, batch nll 0.175991, batch error rate 8.000000%\n",
      "At minibatch 417900, batch loss 0.090303, batch nll 0.059061, batch error rate 0.000000%\n",
      "At minibatch 418000, batch loss 0.659452, batch nll 0.628209, batch error rate 24.000000%\n",
      "After epoch 209: valid_err_rate: 7.930000% currently going to do 211 epochs\n",
      "After epoch 209: averaged train_err_rate: 8.646000% averaged train nll: 0.272145 averaged train loss: 0.303387\n",
      "At minibatch 418100, batch loss 0.273324, batch nll 0.242081, batch error rate 8.000000%\n",
      "At minibatch 418200, batch loss 0.118738, batch nll 0.087496, batch error rate 4.000000%\n",
      "At minibatch 418300, batch loss 0.197248, batch nll 0.166005, batch error rate 8.000000%\n",
      "At minibatch 418400, batch loss 0.407915, batch nll 0.376673, batch error rate 8.000000%\n",
      "At minibatch 418500, batch loss 0.169662, batch nll 0.138419, batch error rate 4.000000%\n",
      "At minibatch 418600, batch loss 0.235447, batch nll 0.204205, batch error rate 8.000000%\n",
      "At minibatch 418700, batch loss 0.130010, batch nll 0.098768, batch error rate 4.000000%\n",
      "At minibatch 418800, batch loss 0.852505, batch nll 0.821262, batch error rate 16.000000%\n",
      "At minibatch 418900, batch loss 0.250294, batch nll 0.219052, batch error rate 8.000000%\n",
      "At minibatch 419000, batch loss 0.314833, batch nll 0.283591, batch error rate 4.000000%\n",
      "At minibatch 419100, batch loss 0.169922, batch nll 0.138680, batch error rate 8.000000%\n",
      "At minibatch 419200, batch loss 0.228677, batch nll 0.197435, batch error rate 4.000000%\n",
      "At minibatch 419300, batch loss 0.218307, batch nll 0.187065, batch error rate 4.000000%\n",
      "At minibatch 419400, batch loss 0.256989, batch nll 0.225746, batch error rate 12.000000%\n",
      "At minibatch 419500, batch loss 0.204643, batch nll 0.173401, batch error rate 12.000000%\n",
      "At minibatch 419600, batch loss 0.366824, batch nll 0.335581, batch error rate 8.000000%\n",
      "At minibatch 419700, batch loss 0.300146, batch nll 0.268904, batch error rate 4.000000%\n",
      "At minibatch 419800, batch loss 0.236069, batch nll 0.204826, batch error rate 8.000000%\n",
      "At minibatch 419900, batch loss 0.299431, batch nll 0.268188, batch error rate 8.000000%\n",
      "At minibatch 420000, batch loss 0.339923, batch nll 0.308681, batch error rate 8.000000%\n",
      "After epoch 210: valid_err_rate: 7.930000% currently going to do 211 epochs\n",
      "After epoch 210: averaged train_err_rate: 8.650000% averaged train nll: 0.272161 averaged train loss: 0.303403\n",
      "At minibatch 420100, batch loss 0.542501, batch nll 0.511259, batch error rate 28.000000%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/i258328/nn_assignments/libs/Theano/theano/tensor/nlinalg.py:321: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  w[0], v[0] = [z.astype(x.dtype) for z in self._numop(x)]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-52-58461ee51f67>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mmomentum\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.9\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0mL\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merr_rate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnll\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwdec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_step2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlrate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/i258328/nn_assignments/libs/Theano/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    857\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    858\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 859\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    860\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'position_of_error'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/i258328/nn_assignments/libs/Theano/theano/scan_module/scan_op.pyc\u001b[0m in \u001b[0;36mrval\u001b[1;34m(p, i, o, n, allow_gc)\u001b[0m\n\u001b[0;32m    961\u001b[0m         def rval(p=p, i=node_input_storage, o=node_output_storage, n=node,\n\u001b[0;32m    962\u001b[0m                  allow_gc=allow_gc):\n\u001b[1;32m--> 963\u001b[1;33m             \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    964\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    965\u001b[0m                 \u001b[0mcompute_map\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/i258328/nn_assignments/libs/Theano/theano/scan_module/scan_op.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(node, args, outs)\u001b[0m\n\u001b[0;32m    950\u001b[0m                         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    951\u001b[0m                         \u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 952\u001b[1;33m                         self, node)\n\u001b[0m\u001b[0;32m    953\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mImportError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtheano\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgof\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMissingGXX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    954\u001b[0m             \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/i258328/nn_assignments/libs/Theano/theano/scan_module/scan_perform.pyx\u001b[0m in \u001b[0;36mtheano.scan_module.scan_perform.perform (/tmp/i258328/theano.NOBACKUP/nn_compiledir_Linux-3.13.0-32-generic-x86_64-with-debian-jessie-sid-x86_64-2.7.10-64/scan_perform/mod.cpp:4193)\u001b[1;34m()\u001b[0m\n\u001b[0;32m    395\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    396\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 397\u001b[1;33m             \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    398\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'position_of_error'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/i258328/nn_assignments/libs/Theano/theano/gof/op.pyc\u001b[0m in \u001b[0;36mrval\u001b[1;34m(p, i, o, n)\u001b[0m\n\u001b[0;32m    876\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mparams\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNoParams\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    877\u001b[0m             \u001b[1;31m# default arguments are stored in the closure of `rval`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 878\u001b[1;33m             \u001b[1;32mdef\u001b[0m \u001b[0mrval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode_input_storage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode_output_storage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    879\u001b[0m                 \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    880\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#number_of_epochs = number_of_epochs * 1.5\n",
    "number_of_epochs = number_of_epochs + 3\n",
    "\n",
    "while e<number_of_epochs: #This loop goes over epochs\n",
    "    e += 1\n",
    "    #First train on all data from this batch\n",
    "    epoch_start_i = i\n",
    "    for X_batch, Y_batch in mnist_train_stream.get_epoch_iterator(): \n",
    "        i += 1\n",
    "\n",
    "        K = 2000\n",
    "        lrate = 4e-3 * K / np.maximum(K, i)\n",
    "        momentum=0.9\n",
    "\n",
    "        L, err_rate, nll, wdec = train_step2(X_batch, Y_batch, lrate, momentum)\n",
    "\n",
    "\n",
    "        train_loss.append((i,L))\n",
    "        train_erros.append((i,err_rate))\n",
    "        train_nll.append((i,nll))\n",
    "        if i % 100 == 0:\n",
    "            print \"At minibatch %d, batch loss %f, batch nll %f, batch error rate %f%%\" % (i, L, nll, err_rate*100)\n",
    "\n",
    "    # After an epoch compute validation error\n",
    "    val_error_rate = compute_error_rate(mnist_validation_stream)\n",
    "    if val_error_rate < best_valid_error_rate:\n",
    "        number_of_epochs = np.maximum(number_of_epochs, e * patience_expansion+1)\n",
    "        best_valid_error_rate = val_error_rate\n",
    "        best_params = snapshot_parameters()\n",
    "        best_params_epoch = e\n",
    "    validation_errors.append((i,val_error_rate))\n",
    "    print \"After epoch %d: valid_err_rate: %f%% currently going to do %d epochs\" %(\n",
    "        e, val_error_rate*100, number_of_epochs)\n",
    "    print \"After epoch %d: averaged train_err_rate: %f%% averaged train nll: %f averaged train loss: %f\" %(\n",
    "        e, np.mean(np.asarray(train_erros)[epoch_start_i:,1])*100, \n",
    "        np.mean(np.asarray(train_nll)[epoch_start_i:,1]),\n",
    "        np.mean(np.asarray(train_loss)[epoch_start_i:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting network parameters from after epoch 138\n",
      "Test error rate is 8.710000%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f34edf22690>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEDCAYAAAAvNJM9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f350d897cd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print \"Setting network parameters from after epoch %d\" %(best_params_epoch)\n",
    "load_parameters(best_params)\n",
    "\n",
    "print \"Test error rate is %f%%\" %(compute_error_rate(mnist_test_stream)*100.0,)\n",
    "\n",
    "subplot(2,1,1)\n",
    "train_nll_a = np.array(train_nll)\n",
    "semilogy(train_nll_a[:,0], train_nll_a[:,1], label='batch train nll')\n",
    "legend()\n",
    "\n",
    "subplot(2,1,2)\n",
    "train_erros_a = np.array(train_erros)\n",
    "plot(train_erros_a[:,0], train_erros_a[:,1], label='batch train error rate')\n",
    "validation_errors_a = np.array(validation_errors)\n",
    "plot(validation_errors_a[:,0], validation_errors_a[:,1], label='validation error rate', color='r')\n",
    "ylim(0,0.2)\n",
    "legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
